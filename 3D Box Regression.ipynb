{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T02:51:49.344531Z",
     "start_time": "2017-05-01T10:51:49.304391+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Reshape, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "BIN, OVERLAP = 2, 0.1\n",
    "W = 1.\n",
    "ALPHA = 1.\n",
    "MAX_JIT = 3\n",
    "NORM_H, NORM_W = 224, 224\n",
    "VEHICLES = ['Car', 'Truck', 'Van', 'Tram']\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "label_dir = '/Volumes/Samsung_T5/git/MonoGRNet/data/KittiBox/training/label_2/'\n",
    "image_dir = '/Volumes/Samsung_T5/git/MonoGRNet/data/KittiBox/training/image_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T02:51:50.559709Z",
     "start_time": "2017-05-01T10:51:49.347095+08:00"
    },
    "code_folding": [
     88
    ]
   },
   "outputs": [],
   "source": [
    "def compute_anchors(angle):\n",
    "    anchors = []\n",
    "    \n",
    "    wedge = 2.*np.pi/BIN\n",
    "    l_index = int(angle/wedge)\n",
    "    r_index = l_index + 1\n",
    "    \n",
    "    if (angle - l_index*wedge) < wedge/2 * (1+OVERLAP/2):\n",
    "        anchors.append([l_index, angle - l_index*wedge])\n",
    "        \n",
    "    if (r_index*wedge - angle) < wedge/2 * (1+OVERLAP/2):\n",
    "        anchors.append([r_index%BIN, angle - r_index*wedge])\n",
    "        \n",
    "    return anchors\n",
    "\n",
    "def compute_angle(anchors):\n",
    "    pass\n",
    "\n",
    "def parse_annotation(label_dir, image_dir):\n",
    "    all_objs = []\n",
    "    dims_avg = {key:np.array([0, 0, 0]) for key in VEHICLES}\n",
    "    dims_cnt = {key:0 for key in VEHICLES}\n",
    "        \n",
    "    for label_file in os.listdir(label_dir):\n",
    "        image_file = label_file.replace('txt', 'png')\n",
    "\n",
    "        for line in open(label_dir + label_file).readlines():\n",
    "            line = line.strip().split(' ')\n",
    "            truncated = np.abs(float(line[1]))\n",
    "            occluded  = np.abs(float(line[2]))\n",
    "\n",
    "            if line[0] in VEHICLES and truncated < 0.1 and occluded < 0.1:\n",
    "                new_alpha = float(line[3]) + np.pi/2.\n",
    "                if new_alpha < 0:\n",
    "                    new_alpha = new_alpha + 2.*np.pi\n",
    "                new_alpha = new_alpha - int(new_alpha/(2.*np.pi))*(2.*np.pi)\n",
    "\n",
    "                obj = {'name':line[0],\n",
    "                       'image':image_file,\n",
    "                       'xmin':int(float(line[4])),\n",
    "                       'ymin':int(float(line[5])),\n",
    "                       'xmax':int(float(line[6])),\n",
    "                       'ymax':int(float(line[7])),\n",
    "                       'dims':np.array([float(number) for number in line[8:11]]),\n",
    "                       'new_alpha': new_alpha\n",
    "                      }\n",
    "                \n",
    "                dims_avg[obj['name']]  = dims_cnt[obj['name']]*dims_avg[obj['name']] + obj['dims']\n",
    "                dims_cnt[obj['name']] += 1\n",
    "                dims_avg[obj['name']] /= dims_cnt[obj['name']]\n",
    "\n",
    "                all_objs.append(obj)\n",
    "            \n",
    "    return all_objs, dims_avg\n",
    "\n",
    "all_objs, dims_avg = parse_annotation(label_dir, image_dir)\n",
    "\n",
    "for obj in all_objs:\n",
    "    # Fix dimensions\n",
    "    obj['dims'] = obj['dims'] - dims_avg[obj['name']]\n",
    "    \n",
    "    # Fix orientation and confidence for no flip\n",
    "    orientation = np.zeros((BIN,2))\n",
    "    confidence = np.zeros(BIN)\n",
    "    \n",
    "    anchors = compute_anchors(obj['new_alpha'])\n",
    "    \n",
    "    for anchor in anchors:\n",
    "        orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n",
    "        confidence[anchor[0]] = 1.\n",
    "        \n",
    "    confidence = confidence / np.sum(confidence)\n",
    "        \n",
    "    obj['orient'] = orientation\n",
    "    obj['conf'] = confidence\n",
    "        \n",
    "    # Fix orientation and confidence for flip\n",
    "    orientation = np.zeros((BIN,2))\n",
    "    confidence = np.zeros(BIN)\n",
    "    \n",
    "    anchors = compute_anchors(2.*np.pi - obj['new_alpha'])\n",
    "    \n",
    "    for anchor in anchors:\n",
    "        orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n",
    "        confidence[anchor[0]] = 1\n",
    "        \n",
    "    confidence = confidence / np.sum(confidence)\n",
    "        \n",
    "    obj['orient_flipped'] = orientation\n",
    "    obj['conf_flipped'] = confidence\n",
    "\n",
    "def prepare_input_and_output(train_inst):\n",
    "    ### Prepare image patch\n",
    "    xmin = train_inst['xmin'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n",
    "    ymin = train_inst['ymin'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n",
    "    xmax = train_inst['xmax'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n",
    "    ymax = train_inst['ymax'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n",
    "    \n",
    "    img = cv2.imread(image_dir + train_inst['image'])\n",
    "    img = copy.deepcopy(img[ymin:ymax+1,xmin:xmax+1]).astype(np.float32)\n",
    "    \n",
    "    # re-color the image\n",
    "    #img += np.random.randint(-2, 3, img.shape).astype('float32')\n",
    "    #t  = [np.random.uniform()]\n",
    "    #t += [np.random.uniform()]\n",
    "    #t += [np.random.uniform()]\n",
    "    #t = np.array(t)\n",
    "\n",
    "    #img = img * (1 + t)\n",
    "    #img = img / (255. * 2.)\n",
    "\n",
    "    # flip the image\n",
    "    flip = np.random.binomial(1, .5)\n",
    "    if flip > 0.5: img = cv2.flip(img, 1)\n",
    "        \n",
    "    # resize the image to standard size\n",
    "    img = cv2.resize(img, (NORM_H, NORM_W))\n",
    "    img = img - np.array([[[103.939, 116.779, 123.68]]])\n",
    "    #img = img[:,:,::-1]\n",
    "    \n",
    "    ### Fix orientation and confidence\n",
    "    if flip > 0.5:\n",
    "        return img, train_inst['dims'], train_inst['orient_flipped'], train_inst['conf_flipped']\n",
    "    else:\n",
    "        return img, train_inst['dims'], train_inst['orient'], train_inst['conf']\n",
    "\n",
    "def data_gen(all_objs, batch_size):\n",
    "    num_obj = len(all_objs)\n",
    "    \n",
    "    keys = list(range(num_obj))\n",
    "    np.random.shuffle(keys)\n",
    "    \n",
    "    l_bound = 0\n",
    "    r_bound = batch_size if batch_size < num_obj else num_obj\n",
    "    \n",
    "    while True:\n",
    "        if l_bound == r_bound:\n",
    "            l_bound  = 0\n",
    "            r_bound = batch_size if batch_size < num_obj else num_obj\n",
    "            np.random.shuffle(keys)\n",
    "        \n",
    "        currt_inst = 0\n",
    "        x_batch = np.zeros((r_bound - l_bound, 224, 224, 3))\n",
    "        d_batch = np.zeros((r_bound - l_bound, 3))\n",
    "        o_batch = np.zeros((r_bound - l_bound, BIN, 2))\n",
    "        c_batch = np.zeros((r_bound - l_bound, BIN))\n",
    "        \n",
    "        for key in keys[l_bound:r_bound]:\n",
    "            # augment input image and fix object's orientation and confidence\n",
    "            image, dimension, orientation, confidence = prepare_input_and_output(all_objs[key])\n",
    "            \n",
    "            #plt.figure(figsize=(5,5))\n",
    "            #plt.imshow(image/255./2.); plt.show()\n",
    "            #print dimension\n",
    "            #print orientation\n",
    "            #print confidence\n",
    "            \n",
    "            x_batch[currt_inst, :] = image\n",
    "            d_batch[currt_inst, :] = dimension\n",
    "            o_batch[currt_inst, :] = orientation\n",
    "            c_batch[currt_inst, :] = confidence\n",
    "            \n",
    "            currt_inst += 1\n",
    "                \n",
    "        yield x_batch, [d_batch, o_batch, c_batch]\n",
    "        \n",
    "        l_bound  = r_bound\n",
    "        r_bound = r_bound + batch_size\n",
    "        if r_bound > num_obj: r_bound = num_obj\n",
    "\n",
    "def l2_normalize(x):\n",
    "    return tf.nn.l2_normalize(x, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Construct the regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T02:51:50.907880Z",
     "start_time": "2017-05-01T10:51:50.562231+08:00"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/connor/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/connor/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-796161edcc6f>:172: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "# Construct the network\n",
    "inputs = Input(shape=(224,224,3))\n",
    "# Block 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "dimension   = Dense(512)(x)\n",
    "dimension   = LeakyReLU(alpha=0.1)(dimension)\n",
    "dimension   = Dropout(0.5)(dimension)\n",
    "dimension   = Dense(3)(dimension)\n",
    "dimension   = LeakyReLU(alpha=0.1, name='dimension')(dimension)\n",
    "\n",
    "orientation = Dense(256)(x)\n",
    "orientation = LeakyReLU(alpha=0.1)(orientation)\n",
    "orientation = Dropout(0.5)(orientation)\n",
    "orientation = Dense(BIN*2)(orientation)\n",
    "orientation = LeakyReLU(alpha=0.1)(orientation)\n",
    "orientation = Reshape((BIN,-1))(orientation)\n",
    "orientation = Lambda(l2_normalize, name='orientation')(orientation)\n",
    "\n",
    "confidence  = Dense(256)(x)\n",
    "confidence  = LeakyReLU(alpha=0.1)(confidence)\n",
    "confidence  = Dropout(0.5)(confidence)\n",
    "confidence  = Dense(BIN, activation='softmax', name='confidence')(confidence)\n",
    "\n",
    "model = Model(inputs, outputs=[dimension, orientation, confidence])\n",
    "\n",
    "#model.load_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Kick off the trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T02:51:50.917482Z",
     "start_time": "2017-05-01T10:51:50.909746+08:00"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def orientation_loss(y_true, y_pred):\n",
    "    # Find number of anchors\n",
    "    anchors = tf.reduce_sum(tf.square(y_true), axis=2)\n",
    "    anchors = tf.greater(anchors, tf.constant(0.5))\n",
    "    anchors = tf.reduce_sum(tf.cast(anchors, tf.float32), 1)\n",
    "    \n",
    "    # Define the loss\n",
    "    loss = -(y_true[:,:,0]*y_pred[:,:,0] + y_true[:,:,1]*y_pred[:,:,1])\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "    loss = loss / anchors\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T02:51:51.068857Z",
     "start_time": "2017-05-01T10:51:50.919082+08:00"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "early_stop  = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, mode='min', verbose=1)\n",
    "checkpoint  = ModelCheckpoint('weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs/', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "all_exams  = len(all_objs)\n",
    "trv_split  = int(0.9*all_exams)\n",
    "batch_size = 8\n",
    "np.random.shuffle(all_objs)\n",
    "\n",
    "train_gen = data_gen(all_objs[:trv_split],          batch_size)\n",
    "valid_gen = data_gen(all_objs[trv_split:all_exams], batch_size)\n",
    "\n",
    "train_num = int(np.ceil(trv_split/batch_size))\n",
    "valid_num = int(np.ceil((all_exams - trv_split)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T07:59:34.809443Z",
     "start_time": "2017-04-23T15:57:59.557392+08:00"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connor/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=1398, epochs=500, verbose=1, validation_data=<generator..., validation_steps=156, callbacks=[<keras.ca..., max_queue_size=3)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "   4/1398 [..............................] - ETA: 2:36:02 - loss: 79049.2275 - dimension_loss: 79049.6186 - orientation_loss: -0.7268 - confidence_loss: 0.3354  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-496d79220ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     max_q_size = 3)\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/image-to-3d-bbox-39wHYzzx/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minimizer  = SGD(lr=0.0001)\n",
    "model.compile(optimizer='adam',#minimizer,\n",
    "              loss={\n",
    "                  'dimension': 'mean_squared_error',\n",
    "                  'orientation': orientation_loss,\n",
    "                  'confidence': 'mean_squared_error'\n",
    "              },\n",
    "              loss_weights={'dimension': 1., 'orientation': 1., 'confidence': 1.})\n",
    "model.fit_generator(generator = train_gen, \n",
    "                    steps_per_epoch = train_num, \n",
    "                    epochs = 500, \n",
    "                    verbose = 1, \n",
    "                    validation_data = valid_gen, \n",
    "                    validation_steps = valid_num, \n",
    "                    callbacks = [early_stop, checkpoint, tensorboard], \n",
    "                    max_q_size = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress 3D boxes on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T03:30:45.190841Z",
     "start_time": "2017-05-01T11:30:45.182191+08:00"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('./weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T03:51:07.574745Z",
     "start_time": "2017-05-01T11:48:43.975138+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.578128695548747 []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-39238f484ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                             \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                             \u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrot_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrot_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                             \u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrot_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrot_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                             \u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "image_dir = '/Volumes/Samsung_T5/Data/2011_09_26/2011_09_26_drive_0009_sync/image_02/data/'\n",
    "box2d_loc = '/Volumes/Samsung_T5/Data/2011_09_26/2011_09_26_drive_0009_sync/box_2d/'\n",
    "box3d_loc = '/Volumes/Samsung_T5/Data/2011_09_26/2011_09_26_drive_0009_sync/box_3d/'\n",
    "\n",
    "all_image = sorted(os.listdir(image_dir))\n",
    "#np.random.shuffle(all_image)\n",
    "\n",
    "for f in all_image:\n",
    "    image_file = image_dir + f\n",
    "    box2d_file = box2d_loc + f.replace('png', 'txt')\n",
    "    box3d_file = box3d_loc + f.replace('png', 'txt')\n",
    "    \n",
    "    with open(box3d_file, 'w') as box3d:\n",
    "        img = cv2.imread(image_file)\n",
    "        img = img.astype(np.float32, copy=False)\n",
    "\n",
    "        for line in open(box2d_file):\n",
    "            line = line.strip().split(' ')\n",
    "            truncated = np.abs(float(line[1]))\n",
    "            occluded  = np.abs(float(line[2]))\n",
    "\n",
    "            obj = {\n",
    "                'xmin':int(float(line[4])),\n",
    "                'ymin':int(float(line[5])),\n",
    "                'xmax':int(float(line[6])),\n",
    "                'ymax':int(float(line[7]))\n",
    "            }\n",
    "\n",
    "            patch = img[obj['ymin']:obj['ymax'],obj['xmin']:obj['xmax']]\n",
    "            patch = cv2.resize(patch, (NORM_H, NORM_W))\n",
    "            patch = patch - np.array([[[103.939, 116.779, 123.68]]])\n",
    "            patch = np.expand_dims(patch, 0)\n",
    "\n",
    "            prediction = model.predict(patch)\n",
    "\n",
    "            # Transform regressed angle\n",
    "            max_anc = np.argmax(prediction[2][0])\n",
    "            anchors = prediction[1][0][max_anc]\n",
    "\n",
    "            if anchors[1] > 0:\n",
    "                angle_offset = np.arccos(anchors[0])\n",
    "            else:\n",
    "                angle_offset = -np.arccos(anchors[0])\n",
    "\n",
    "            wedge = 2. * np.pi / BIN\n",
    "            angle_offset = angle_offset + max_anc * wedge\n",
    "            angle_offset = angle_offset % (2. * np.pi)\n",
    "\n",
    "            angle_offset = angle_offset - np.pi / 2\n",
    "            if angle_offset > np.pi:\n",
    "                angle_offset = angle_offset - (2. * np.pi)\n",
    "\n",
    "            line[3] = str(angle_offset)\n",
    "\n",
    "            # Transform regressed dimension\n",
    "            dims = dims_avg['Car'] + prediction[0][0]\n",
    "            line = line + list(dims)\n",
    "\n",
    "            # Write regressed 3D dim and oritent to file\n",
    "            line = ' '.join([str(item) for item in line]) + '\\n'\n",
    "            box3d.write(line)\n",
    "            \n",
    "#             cv2.rectangle(img, (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img/255.); plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "image-to-3d-bbox",
   "language": "python",
   "name": "image-to-3d-bbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "83px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
